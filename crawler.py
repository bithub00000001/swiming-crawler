import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import asyncio
from telegram import Bot
import time

# GitHub Secretsì—ì„œ í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
BOT_TOKEN = os.environ['TELEGRAM_BOT_TOKEN']
CHAT_ID = os.environ['TELEGRAM_CHAT_ID']
SCRAPER_API_KEY = os.environ['SCRAPER_API_KEY']


async def send_telegram_message(message, use_html=True):
    bot = Bot(token=BOT_TOKEN)
    try:
        parse_mode = 'HTML' if use_html else None
        await bot.send_message(
            chat_id=CHAT_ID,
            text=message,
            parse_mode=parse_mode
        )
    except Exception as e:
        if use_html:
            await send_telegram_message(message, use_html=False)
        else:
            print(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")


def crawl_notices_with_scraperapi():
    """ScraperAPIë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
    print("ScraperAPIë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ ì‹œì‘...")

    # ScraperAPI ìš”ì²­ íŒŒë¼ë¯¸í„° [web:229][web:231]
    payload = {
        'api_key': SCRAPER_API_KEY,
        'url': 'https://www.jjss.or.kr/reserv/planweb/board/list.9is?contentUid=ff8080816c5f9de6016cd702efc70de1&boardUid=ff8080816d4d1c03016d85eb2aff02cd&categoryUid2=C1',
        'country_code': 'kr',  # í•œêµ­ IP ì‚¬ìš© [web:231]
        'follow_redirect': 'true',
        'render': 'false',  # JavaScript ë Œë”ë§ ë¶ˆí•„ìš”
        'timeout': '30000',  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
        'retry_404': 'true'
    }

    try:
        response = requests.get(
            'https://api.scraperapi.com/',
            params=payload,
            timeout=60  # ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ
        )

        print(f"ScraperAPI ì‘ë‹µ ìƒíƒœ: {response.status_code}")

        if response.status_code == 200:
            return parse_notices_from_html(response.text)
        else:
            print(f"ScraperAPI ì˜¤ë¥˜: {response.status_code} - {response.text}")
            raise Exception(f"ScraperAPI ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

    except Exception as e:
        print(f"ScraperAPI í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        raise


def parse_notices_from_html(html_content):
    """HTMLì—ì„œ ê³µì§€ì‚¬í•­ ì¶”ì¶œ"""
    print("HTML íŒŒì‹± ì‹œì‘...")

    soup = BeautifulSoup(html_content, 'html.parser')
    notices = []

    # ê³µì§€ì‚¬í•­ í…Œì´ë¸” ì°¾ê¸°
    table = soup.find('table', class_='bbsList bbs01')
    if not table:
        print("ê³µì§€ì‚¬í•­ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë°›ì€ HTML ì¼ë¶€:")
        print(html_content[:500])
        return notices

    tbody = table.find('tbody')
    if not tbody:
        print("í…Œì´ë¸” ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return notices

    rows = tbody.find_all('tr')
    print(f"ì´ {len(rows)}ê°œ í–‰ ë°œê²¬")

    for i, row in enumerate(rows):
        cells = row.find_all('td')
        if len(cells) >= 6:
            try:
                # ì œëª© ì¶”ì¶œ
                title_cell = cells[2]
                title_link = title_cell.find('a')
                if title_link:
                    title_span = title_link.find('span')
                    if title_span:
                        title = title_span.get_text(strip=True)

                        # ë§í¬ ì¶”ì¶œ
                        href = title_link.get('href', '')
                        if href.startswith('./'):
                            href = href[2:]  # ./ ì œê±°
                        link = f'https://www.jjss.or.kr/reserv/planweb/board/{href}'

                        # ë‚ ì§œ ì¶”ì¶œ
                        date = cells[4].get_text(strip=True)

                        # "ì‹ ê·œ" ë˜ëŠ” "ì´ˆê¸‰" í‚¤ì›Œë“œ í•„í„°ë§
                        if "ì‹ ê·œ" in title or "ì´ˆê¸‰" in title:
                            notices.append({
                                'title': title,
                                'link': link,
                                'date': date
                            })
                            print(f"âœ“ ë°œê²¬ëœ ê³µì§€ [{i + 1}]: {title}")
                        else:
                            print(f"  ì¼ë°˜ ê³µì§€ [{i + 1}]: {title}")
            except Exception as e:
                print(f"í–‰ íŒŒì‹± ì˜¤ë¥˜ [{i + 1}]: {e}")
                continue

    print(f"ì´ {len(notices)}ê°œ ì‹ ê·œ/ì´ˆê¸‰ ê³µì§€ ë°œê²¬")
    return notices


async def main():
    start_time = datetime.now()
    try:
        print("=" * 50)
        print(f"ì™„ì‚°ìˆ˜ì˜ì¥ ì•Œë¦¼ë´‡ ì‹œì‘ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 50)

        # ScraperAPIë¡œ í¬ë¡¤ë§
        current_notices = crawl_notices_with_scraperapi()

        # ì´ì „ ê³µì§€ì‚¬í•­ ë°ì´í„° ë¡œë“œ
        try:
            with open('data/last_posts.json', 'r', encoding='utf-8') as f:
                last_notices = json.load(f)
        except FileNotFoundError:
            print("ì´ì „ ë°ì´í„° ì—†ìŒ. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            last_notices = []

        # ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ì°¾ê¸°
        last_titles = {notice['title'] for notice in last_notices}
        new_notices = [notice for notice in current_notices
                       if notice['title'] not in last_titles]

        print(f"ì „ì²´ ì‹ ê·œ/ì´ˆê¸‰ ê³µì§€: {len(current_notices)}ê°œ")
        print(f"ìƒˆë¡œìš´ ê³µì§€: {len(new_notices)}ê°œ")

        # ìƒˆ ê³µì§€ì‚¬í•­ì´ ìˆìœ¼ë©´ í…”ë ˆê·¸ë¨ ì „ì†¡
        if new_notices:
            print("ìƒˆ ê³µì§€ì‚¬í•­ í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹œì‘...")
            for i, notice in enumerate(new_notices):
                message = f"""
ğŸŠâ€â™€ï¸ <b>ì™„ì‚°ìˆ˜ì˜ì¥ ì‹ ê·œ/ì´ˆê¸‰ ê³µì§€</b>

ğŸ“‹ <b>{notice['title']}</b>
ğŸ“… ë“±ë¡ì¼: {notice['date']}
ğŸ”— <a href="{notice['link']}">ê³µì§€ì‚¬í•­ ë³´ê¸°</a>

ğŸ¤– ScraperAPIë¥¼ í†µí•œ ìë™ ì•Œë¦¼
                """.strip()

                await send_telegram_message(message)
                print(f"  âœ“ ì „ì†¡ ì™„ë£Œ [{i + 1}/{len(new_notices)}]: {notice['title']}")

                if i < len(new_notices) - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
                    time.sleep(2)
        else:
            # ì •ìƒ ì‘ë™ í™•ì¸ ë©”ì‹œì§€
            end_time = datetime.now()
            duration = (end_time - start_time).seconds

            success_message = f"""
âœ… <b>ì™„ì‚°ìˆ˜ì˜ì¥ ì•Œë¦¼ë´‡ ì •ìƒ ì‘ë™</b>

ğŸ” ì‹ ê·œ/ì´ˆê¸‰ ê³µì§€ì‚¬í•­ {len(current_notices)}ê°œ í™•ì¸ ì™„ë£Œ
â±ï¸ ì‹¤í–‰ì‹œê°„: {duration}ì´ˆ
ğŸ“… í™•ì¸ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
ğŸ”„ ë‹¤ìŒ í™•ì¸: 2ì‹œê°„ í›„

ğŸŒ ScraperAPI ì‚¬ìš© (í•œêµ­ IP)
            """.strip()
            await send_telegram_message(success_message)

        # í˜„ì¬ ê³µì§€ì‚¬í•­ ì €ì¥
        os.makedirs('data', exist_ok=True)
        with open('data/last_posts.json', 'w', encoding='utf-8') as f:
            json.dump(current_notices, f, ensure_ascii=False, indent=2)

        print("=" * 50)
        print("ì‘ì—… ì™„ë£Œ!")
        print("=" * 50)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        error_message = f"""âŒ ì™„ì‚°ìˆ˜ì˜ì¥ ì•Œë¦¼ë´‡ ì˜¤ë¥˜ ë°œìƒ

ğŸ”§ ScraperAPI ì‚¬ìš© ì¤‘ ì˜¤ë¥˜
âš ï¸ ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:150]}...
ğŸ“… ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ”„ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë‹¤ì‹œ ì‹œë„ë©ë‹ˆë‹¤.
ğŸ’¡ ì§€ì†ì  ì˜¤ë¥˜ ì‹œ GitHub Actions ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.
        """.strip()

        await send_telegram_message(error_message, use_html=False)


if __name__ == "__main__":
    asyncio.run(main())
